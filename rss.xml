<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Gerardo's musings]]></title><description><![CDATA[Gerardo's musings]]></description><link>http://www.gerardomoad.com/</link><generator>metalsmith-feed</generator><lastBuildDate>Sun, 21 Feb 2016 09:40:35 GMT</lastBuildDate><atom:link href="http://www.gerardomoad.com/rss.xml" rel="self" type="application/rss+xml"/><author><![CDATA[Gerardo Moad]]></author><item><title><![CDATA[No title]]></title><description><![CDATA[<h1 id="deploying-my-metalsmith-generated-github-pages-hosted-personal-blog-from-the-comfort-of-git-push-">Deploying my Metalsmith-generated, Github-Pages-hosted personal blog from the comfort of <code>git push</code></h1>
<p>I&#39;m setting up Travis to automatically deploy my Metalsmith-generated blog files.</p>
<ul>
<li>My blog sources (Markdown and Metalsmith scripts to pregenerate html) repo are at:
<code>sanx/sanx-blog-source</code> (master branch)</li>
<li>The github-pages repo whose files get published to my user pages is:
<code>sanx/sanx.github.io</code> (master branch)</li>
</ul>
<p>Steps:</p>
<ul>
<li><code>cd ~/sanx-blog-source</code></li>
<li><code>ssh-keygen -f</code></li>
<li>Github web, repo <code>sanx/sanx.github.io</code>. Go to <em>repo</em> Settings -&gt; Deploy keys. Add
a newly generated key. Paste public key. Check &quot;Allow write access&quot;. Click on
&quot;Add key&quot; to save.</li>
</ul>
<p>Next steps (setup travis build and store encrypted deploy ssh keys) are based on <a href="https://docs.travis-ci.com/user/encrypting-files/">https://docs.travis-ci.com/user/encrypting-files/</a>:</p>
<ul>
<li>Check that <code>travis -v</code> is <code>1.7.0</code> or later.</li>
<li><code>travis init</code> (since I didn&#39;t have a <code>.travis.yml</code> already)</li>
<li><code>travis login</code></li>
<li><code>travis encrypt-file sanx.github.io.travis --add</code> will generate <code>travis encrypt-file sanx.github.io.travis.enc</code>
and modify <code>.travis.yml</code> to make the encryption key available to our build.</li>
</ul>
<p>Add this to <code>.travis.yml</code>:</p>
<pre><code>deploy:
  provider: script
  script: bin/sanx.github.io-deploy.sh
  on:
    branch: master
</code></pre><p>Create file <code>bin/sanx.github.io-deploy.sh</code>:</p>
<pre><code>#!/usr/bin/env bash

DIR=&quot;$( cd &quot;$( dirname &quot;${BASH_SOURCE[0]}&quot; )&quot; &amp;&amp; pwd )&quot;

npm install
node index.js

rm -rf sanx.github.io
mkdir -p sanx.github.io || { echo &quot;fail&quot;; exit 1; }
cd sanx.github.io || { echo &quot;fail&quot;; exit 1; }
printf &quot;Host github.com \n IdentityFile $DIR/../sanx.github.io.travis\n&quot; &gt;&gt; ~/.ssh/config
git clone git@github.com:sanx/sanx.github.io.git .

cp -r ../_site/* .
git commit -m &quot;auto commit at: `date`&quot; -a
git push
</code></pre><p>Last steps before trying it all on the Travis server:</p>
<ul>
<li><code>git add sanx.github.io.travis.enc</code> important <em>not</em> to add the unencrypted private key!</li>
<li><code>git add .travis.yml</code></li>
<li><code>git add bin/sanx.github.io-deploy.sh</code></li>
</ul>
<p>Ta-da! I should be all set for my blog&#39;s static html pages to be re-generated
by Metalsmith running on Travis on every commit that I push to my blog sources repo.</p>
]]></description><link>http://www.gerardomoad.com/2016-02-20-deploying-metalsmith-blog-with-travis</link><guid isPermaLink="true">http://www.gerardomoad.com/2016-02-20-deploying-metalsmith-blog-with-travis</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Sat, 20 Feb 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Moved blog to Metalsmith.io]]></title><description><![CDATA[<p>Finally a blog I can call my own! I can now say that I understand the inner
workings of the platform that my blog runs on (or rather, is statically
generated by), since I made it myself using excellent open source tools, such
as: <a href="http://www.metalsmith.io">Metalsmith.io</a> (and a bunch of ready-made,
community-contributed plugins made for it), and <a href="http://sass-lang.com">Sass</a>
with <a href="http://susy.oddbird.net">Susy</a> for easily making a responsive design
that looks good, at least in my design-challenged eyes :-).</p>
<p>It all began when I started investigating about blogging platform options for
Node.js, shortly before I rebooted this blog using Jekyll/GitHub Pages, and I
kept on hearing about how great Metalsmith.io was for static site generation.</p>
<p>Then, one day, I finally decided to pull the plug on Jekyll and try to re-tool
my blog with <a href="http://www.metalsmith.io">Metalsmith.io</a>. An <a href="https://github.com/segmentio/metalsmith/tree/master/examples/jekyll">implicit promise on
Metalsmith.io&#39;s examples directory</a>,
where a claim is made that you can port a Jekyll blog and easily have it work
under Metalsmith.io (or at least I understood, or wanted to understand it that way :-) ).</p>
<p>Porting my blog helped me learn a lot about Metalsmith.io, the philosophy behind
it, and its pitfalls. I&#39;m also proud to say that one of the plugins that I
developed to fill in a gap for a feature that I was relying on Jekyll/GitHub pages,
namely: the date of posts is picked up from the filename, as opposed to having
to have it in the YAML front-matter at the beginning of posts, is now listed
on <a href="http://www.metalsmith.io/#the-plugins" title="look for &#39;Date in Filename&#39;">Metalsmith.io&#39;s plugins section</a>.</p>
<p>This blog continues to be hosted on the amazing GitHub pages, but now just as
static files, as opposed to relying on it to run the static site generation.</p>
<p>With that, I hope that now I&#39;ll blog more often about topics that I&#39;ve been playing
with a lot lately, like React.js, Lodash, and Metalsmith.io itself.</p>
<p>Cheers!</p>
<p>Gerardo.</p>
]]></description><link>http://www.gerardomoad.com/posts/moved-blog-to-metalsmith-io</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/moved-blog-to-metalsmith-io</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Fri, 07 Nov 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Safely daemonizing a NodeJS server in Ubuntu]]></title><description><![CDATA[<p>We don&#39;t want to run the NodeJS service as root just to be able to bind to
port 80, given all the risks that doing this would bring.</p>
<h2 id="use-iptables-to-forward-requests-coming-in-on-port-80-over-to-port-3000">Use <code>iptables</code> to forward requests coming in on port 80 over to port 3000</h2>
<pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> iptables -t nat -A PREROUTING -i eth0 -p tcp --dport <span class="hljs-number">80</span> -j REDIRECT --to-port <span class="hljs-number">3000</span></code></pre>

<pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> ip6tables -t nat -A PREROUTING -i eth0 -p tcp --dport <span class="hljs-number">80</span> -j REDIRECT --to-port <span class="hljs-number">3000</span></code></pre>

<p>... this will work until you restart the server.</p>
<h2 id="use-iptables-persistent-to-restore-the-iptables-routes-automatically-on-every-reboot">Use <code>iptables-persistent</code> to restore the <code>iptables</code> routes automatically on every reboot</h2>
<pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> apt-get install iptables-persistent</code></pre>

<p>... if you did this after the <code>iptables</code> command above it, just answer <em>yes</em> and <em>yes</em>
to save your forwarding rules in package <code>iptables-persistent</code> post-install script.</p>
<p>If, for some reason, your routes were not quite working the way you wanted them to
before you installed package <code>iptables-persistent</code>, go ahead and tweak them to your
liking, and then run the following commands to save them:</p>
<pre><code></code></pre>

<pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> ip6tables-save &gt; /etc/iptables/rules.v6</code></pre>

<p>That&#39;s it! <code>iptables-persistent</code> is a service that should load <code>/etc/iptables/rules.v{4,6}</code>
into <code>iptables</code> on system startup!</p>
<h2 id="use-upstart-to-daemonize-your-nodejs-service">Use <code>upstart</code> to &quot;daemonize&quot; your <code>NodeJS</code> service</h2>
<p>cat /home/ubuntu/myservice/server.sh:</p>
<pre><code class="hljs bash"><span class="hljs-shebang">#!/bin/bash
</span>
    SCRIPT_PATH=$(dirname `which <span class="hljs-variable">$0</span>`)

    node --harmony <span class="hljs-variable">$SCRIPT_PATH</span>/index.js</code></pre>

<p>cat /etc/init/myservice.conf:</p>
<pre><code class="hljs bash"><span class="hljs-comment"># cat /etc/init/myservice.conf</span>
    <span class="hljs-comment"># http://upstart.ubuntu.com/wiki/Stanzas</span>

    description <span class="hljs-string">"My Service"</span>
    author      <span class="hljs-string">"github.com/sanx"</span>

    stop on shutdown
    respawn
    respawn limit <span class="hljs-number">20</span> <span class="hljs-number">5</span>

    <span class="hljs-comment"># Max open files are @ 1024 by default. Bit few.</span>
    limit nofile <span class="hljs-number">32768</span> <span class="hljs-number">32768</span>

    script
      <span class="hljs-keyword">set</span> <span class="hljs-operator">-e</span>
      mkfifo /tmp/myservice-log-fifo
      ( logger -t myservice &lt;/tmp/myservice-log-fifo &amp; )
      <span class="hljs-keyword">exec</span> &gt;/tmp/myservice-log-fifo
      rm /tmp/myservice-log-fifo
      <span class="hljs-keyword">exec</span> <span class="hljs-built_in">sudo</span> -u www-data MASTERKEY=`<span class="hljs-built_in">echo</span> hi` /home/ubuntu/myservice/server.sh <span class="hljs-number">2</span>&gt;&amp;<span class="hljs-number">1</span>
    end script

    post-start script
       <span class="hljs-built_in">echo</span> <span class="hljs-string">'My Service Just started'</span>
    end script</code></pre>

<p>That&#39;s it! Now, you can start your service now by calling: <code>start myservice</code>, stop
it with <code>stop myservice</code>, and it will run automatically on system startup!</p>
]]></description><link>http://www.gerardomoad.com/posts/safely-daemonizing-a-nodejs-server-in-ubuntu</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/safely-daemonizing-a-nodejs-server-in-ubuntu</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Thu, 21 Aug 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Reconfiguring an installed package in Ubuntu, Debian, and other Debian based distributions.]]></title><description><![CDATA[<p>It&#39;s happened to me several times that I <code>apt-get install</code> some packages which
prompt me to enter some settings in a <code>curses</code> interface, but I don&#39;t know which
values to enter at the time, or later on realize that I need different settings
than what I had originally entered.</p>
<p>To see the screen that you saw again, just do:</p>
<p><code>sudo dpkg-reconfigure PKG_NAME</code></p>
<p>... where <code>PKG_NAME</code> is the name of the package whose configuration screens
you wish to see again.</p>
]]></description><link>http://www.gerardomoad.com/posts/reconfiguring-an-installed-package-in-ubuntu-debian-and-other-debian-based-distributions</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/reconfiguring-an-installed-package-in-ubuntu-debian-and-other-debian-based-distributions</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Wed, 23 Jul 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Receiving and relaying email for my domains over to free webmail accounts.]]></title><description><![CDATA[<h2 id="goal">Goal</h2>
<ul>
<li>Be able to receive email addressed to email addresses in domains that I own
on commercial, free webmail accounts (or any other kind of email system,
really).</li>
<li>Not pay any extra money for this (beyond what I&#39;m paying for domain
registration, DNS hosting, and my server).</li>
</ul>
<h2 id="prerequisites">Prerequisites</h2>
<ul>
<li>Have some domains that you&#39;re in control of, e.g.: the domain resolves
to a host that you control, you have access to the DNS hosting account and
are able to add/remove/modify DNS records (especially MX records).</li>
<li>Have control over a host on the internet where you&#39;ll be pointing some
records of your domains to. For this guide, I&#39;m using Debian-based Ubuntu Linux
Server distribution version 14.04 LTS.</li>
</ul>
<h2 id="steps">Steps</h2>
<ol>
<li><p>create a new MX record with the following information (how you enter this
into your DNS provider&#39;s interface should be easy to figure out):</p>
<pre><code>host name: yourdomain.com
type: MX
ttl: default
priority: 10 (Normal)
data: yourdomain.com
</code></pre></li>
<li><p><code>sudo apt-get install postfix</code></p>
</li>
<li>Choose &quot;Internet site&quot; option on menu prompt and enter settings that make
sense. We&#39;ll do the rest of the configuration manually.</li>
<li><p>Put this at the end of your <code>/etc/postfix/main.cf</code> file:</p>
<pre><code>## added by me
virtual_alias_domains = yourfirstdomain.com, yourseconddomain.com
virtual_alias_maps = hash:/etc/postfix/virtual
inet_protocols = all
</code></pre></li>
<li><p>Create file <code>/etc/postfix/virtual</code> and add lines to it like the following:</p>
<pre><code>name@yourfirstdomain.com        name@freewebmailprovider.com
name@yourseconddomain.com       name@freewebmailprovider.com
</code></pre><p>... this will cause any mail for <code>name@yourfirstdomain.com</code> and 
<code>name@yourseconddomain.com</code> to be forwarded to <code>name@freewebmailprovider.com</code></p>
</li>
<li>Make sure you generate the &quot;hash&quot; or &quot;.db&quot; file for <code>virtual</code> by doing:
<code>sudo postmap hash:/etc/postfix/virtual</code></li>
<li>Reload <code>postfix</code> config with <code>sudo /etc/init.d/postfix reload</code>, or
start <code>postfix</code> in case it is not running already: <code>sudo /etc/init.d/postfix start</code></li>
<li>Make sure that port 25 (SMTP) is open on your server&#39;s ACL/firewall!</li>
<li>Done!</li>
</ol>
<h2 id="troubleshooting">Troubleshooting</h2>
<p>Pretty much anything that might have gone wrong with your <code>postfix</code> configuration
will be logged to either one of these upon the package&#39;s <code>start</code> or <code>reload</code>:</p>
<pre><code>/var/log/mail.err
/var/log/mail.log
</code></pre><h2 id="conclusion">Conclusion</h2>
<p>This comes in very handy for leveraging your ability to support multiple email
addresses on the domains that you control while letting you not worry about
setting up an actual client, and using existing free webmail accounts for
receiving your email.</p>
<p>This approach covers the <em>receiving</em> part very well, but I&#39;ve made no effort
to support sending via your server/hosting, and it&#39;s unlikely to work out of
the box without additional configuration. My suggestion is to not try to use
your server for sending email, and to use your webmail or similar client
with a &quot;Reply-To&quot; header if you want. I don&#39;t really care too much since I
just want the receiving part to work, and later on I can reply to emails that
matter to me using my real email address.</p>
]]></description><link>http://www.gerardomoad.com/posts/receiving-and-relaying-email-for-my-domains-over-to-free-webmail-accounts</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/receiving-and-relaying-email-for-my-domains-over-to-free-webmail-accounts</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Wed, 23 Jul 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Vim settings missing in OSX]]></title><description><![CDATA[<p>I got my new laptop (a 13&#39;&#39; rMBP). Once I started using it, I started noticing
there are a few default settings that an <em>Ubuntu</em> install of <code>vim</code> has that 
were missing in <em>OSX</em> (Mavericks). Here are the few that I&#39;ve added to my
<code>~/.vimrc</code> so far that I&#39;ve found useful:</p>
<pre><code>set ruler
filetype plugin indent on
syntax on
</code></pre><p>... and some additional settings to have tabs and shifts be automatically
converted to 4 spaces:</p>
<pre><code>set expandtab
set shiftwidth=4
set tabstop=4
set softtabstop=4
</code></pre><p>... and finally, I enable modelines, since I like to use them on my source
files and to support them when opening others&#39; files:</p>
<pre><code>set modeline
set modelines=5
</code></pre>]]></description><link>http://www.gerardomoad.com/posts/vim-settings-missing-in-osx</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/vim-settings-missing-in-osx</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Wed, 21 May 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Ruby/Jekyll/Github Pages/OSX madness!]]></title><description><![CDATA[<p>Trying to get my Jekyll blog setup locally on my new Mac, I&#39;ve been following
these steps, and encountering the following issues:</p>
<p>I remembered that I would need to use <code>bundle</code> to launch my Jekyll instance
in order for it to work the same way as it does on the hosted Github Pages, so I
ran:</p>
<pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> gem install bundle</code></pre>

<p>output:</p>
<pre><code>Fetching: bundler-1.6.2.gem (100%)
Successfully installed bundler-1.6.2
Fetching: bundle-0.0.1.gem (100%)
Successfully installed bundle-0.0.1
Parsing documentation for bundler-1.6.2
Installing ri documentation for bundler-1.6.2
Parsing documentation for bundle-0.0.1
Installing ri documentation for bundle-0.0.1
2 gems installed
</code></pre><p>as Borat would say: &quot;nice! I like!&quot;. I like, indeed!</p>
<p>I also remembered that for everything that the hosted Github Pages does
to work locally, I had to install the <code>git-pages</code> <em>gem</em>. I tried doing so:</p>
<pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> gem install github-pages</code></pre>

<p>output:</p>
<pre><code>Fetching: liquid-2.5.5.gem (100%)
Successfully installed liquid-2.5.5
Fetching: fast-stemmer-1.0.2.gem (100%)
Building native extensions.  This could take a while...
ERROR:  Error installing github-pages:
        ERROR: Failed to build gem native extension.

    /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/bin/ruby extconf.rb
creating Makefile

make &quot;DESTDIR=&quot;
compiling porter.c
porter.c:359:27: warning: &#39;&amp;&amp;&#39; within &#39;||&#39; [-Wlogical-op-parentheses]
      if (a &gt; 1 || a == 1 &amp;&amp; !cvc(z, z-&gt;k - 1)) z-&gt;k--;
                ~~ ~~~~~~~^~~~~~~~~~~~~~~~~~~~
porter.c:359:27: note: place parentheses around the &#39;&amp;&amp;&#39; expression to silence this warning
      if (a &gt; 1 || a == 1 &amp;&amp; !cvc(z, z-&gt;k - 1)) z-&gt;k--;
                          ^
                   (                          )
1 warning generated.
compiling porter_wrap.c
linking shared-object stemmer.bundle
clang: error: unknown argument: &#39;-multiply_definedsuppress&#39; [-Wunused-command-line-argument-hard-error-in-future]
clang: note: this will be a hard error (cannot be downgraded to a warning) in the future
make: *** [stemmer.bundle] Error 1


Gem files will remain installed in /Library/Ruby/Gems/2.0.0/gems/fast-stemmer-1.0.2 for inspection.
Results logged to /Library/Ruby/Gems/2.0.0/gems/fast-stemmer-1.0.2/ext/gem_make.out
</code></pre><p>... oopsie, there were at least some native packages that could not be built
on my system. After some investigation, I finally got it working and contributed my solution to <strong>Stack Overflow</strong>
on <a href="http://stackoverflow.com/a/23795873/1369119">here</a>. Damn, it feels good to be a gangster!</p>
<p>Now, I&#39;m able to run <code>bundle exec jekyll serve</code> on my Mac locally, and it fires
up an instance of my <strong>Github Pages</strong>/<strong>Jekyll</strong> blog on port 4000.</p>
]]></description><link>http://www.gerardomoad.com/posts/ruby-jekyll-github-pages-osx-madness</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/ruby-jekyll-github-pages-osx-madness</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Wed, 21 May 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[More tweaks to my Jekyll blog]]></title><description><![CDATA[<p>I&#39;ve made my Jekyll blog a little better (closer to having proper blog features)
by doing the following:</p>
<ol>
<li>I fixed the pagination by using the code found at the bottom of the <a href="http://jekyllrb.com/docs/pagination/">Jekyll
guide page on pagination</a>.</li>
<li>I added RSS support by adapting one of <a href="https://github.com/snaptortoise/jekyll-rss-feeds">the templates I found here</a>.</li>
</ol>
<p>No. 1 on my <strong>TODO</strong> list is a good commenting model that&#39;s compatible with Jekyll&#39;s
static site generation.</p>
]]></description><link>http://www.gerardomoad.com/posts/more-tweaks-to-my-jekyll-blog</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/more-tweaks-to-my-jekyll-blog</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Wed, 21 May 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Blogging with Github Pages and Jekyll]]></title><description><![CDATA[<p>I created a repository named <a href="https://github.com/sanx/sanx.github.io"><code>sanx.github.io</code></a> (<em>sanx</em> is my Github username. this blog&#39;s raw data is on that repo!), then I followed the steps to auto publish a page using a layout that I liked from the ones they have available, then I ventured into trying to get it all to work using the Jekyll blogging pattern, basically porting the templates and the includes so that my site is generated using Jekyll while using the templates that I want.</p>
<p>When I was initially trying to get my Markdown posts to be generated by Jekyll on Github Pages, I started getting build error emails, and basically my site wasn&#39;t being updated at all, which prompted me to follow the steps for installing Jekyll locally to make debugging easier. Oh, the surprise when I ran <code>jekyll</code> or even <code>jekyll serve</code> and I got nice outputs, something like the following for <code>jekyll serve</code>:</p>
<p>Output:</p>
<pre><code>Configuration file: none
            Source: /home/germoad/gerardomoad.com
       Destination: /home/germoad/gerardomoad.com/_site
      Generating... 
                    done.
Configuration file: none
    Server address: http://0.0.0.0:4000/
  Server running... press ctrl-c to stop.
</code></pre><p>... which was confusing, because the build was failing on Github Pages, and I was supposedly using the same Jekyll thingie that it does, locally. Upon reading a little more, I followed the following link from the link in my build errors email: <a href="https://help.github.com/articles/using-jekyll-with-pages#troubleshooting">https://help.github.com/articles/using-jekyll-with-pages#troubleshooting</a> , and in there they mention that the most accurate way of running Jekyll in a way that closely resembles how it&#39;s run by Github Pages is by doing <code>bundle exec jekyll serve</code>. When I ran that command, it gave me a nice debug error pointing to the exact spot on my Markdown document that had a syntax error.</p>
]]></description><link>http://www.gerardomoad.com/posts/blogging-with-github-pages-and-jekyll</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/blogging-with-github-pages-and-jekyll</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Wed, 14 May 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[Setting up `tmux`]]></title><description><![CDATA[<p>Now that I&#39;ve <em>kind of</em> lost (I hope that I haven&#39;t, but I&#39;ve been lazy to unbury it from my backups, plus I like being kind of forced to re-do my setup from scratch) my old *nix setup, including my .bashrc and my tmux setup, I need to re-do my tmux setup file.</p>
<p>I like tmux to act gnu screen-ish (mainly having <code>Ctrl-A</code> as the start of command sequences, and having both <code>Ctrl-A Ctrl-A</code> and <code>Ctrl-A A</code> act as <em>jump to last seen panel</em> sequence. I think that&#39;s all I need from gnu screen that&#39;s initially missing in tmux.</p>
<p><em>Update</em>: A few days after I initially wrote this post, I realized that about the only settings that I need to change in order to feel comfortable using <code>tmux</code> are:</p>
<ul>
<li>the default escape sequence (<code>Ctrl-B</code> by default) to <code>Ctrl-A</code> (a la <em>GNU</em> <code>screen</code>)</li>
<li>use <code>vi</code> keys for navigating the screen (while in copy or command modes)</li>
<li>(bonus) have a nice visual distinction between the currently active window, the last active window, and the rest of the windows on the bottom status bar</li>
</ul>
<p>This was accomplished easily by putting the following <code>.tmux.conf</code> in my home directory:</p>
<pre><code># change the default escape sequence form Ctrl-B to Ctrl-A:
set-option -g prefix C-a
unbind-key C-b
bind-key C-a send-prefix

# use &quot;vi&quot; style screen navigation (as opposed to Emacs, which is what tmux uses by default):
setw -g mode-keys vi
set -g status-keys vi

# make currently active and last active parts of the status bar different from the rest of the windows:
set-option -gw window-status-current-bg &quot;green&quot;
set-option -gw window-status-current-attr &quot;italics&quot;

set-option -gw window-status-last-bg &quot;green&quot;
set-option -gw window-status-last-attr &quot;bold&quot;
</code></pre>]]></description><link>http://www.gerardomoad.com/posts/setting-up-tmux</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/setting-up-tmux</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Tue, 13 May 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[On Macbook Pros]]></title><description><![CDATA[<p>Lately, I&#39;ve been observing people on the commuter train, and one particular type of person that I see are people sporting their corporate issued Macbook Pro laptops. Some of them are typing away at their code editors, others are filling in weekly reports, etc.</p>
<p>It&#39;s interesting how about the only people with Macbook Pros that I see look like they&#39;re using the corporate laptop. I don&#39;t see a lot of people who seem to be using their personal privately bought Macbook Pros (although, admittedly, I think I&#39;ve seen a few people with Macbook Airs and other type of older white Macbooks). A few people who I know had Macbook Pros that they had bought by themselves took care of it as if the laptops were more valuable (or needed more care than, at least) a newborn baby. You know the drill: super padded bag case, another zipped case where the laptop is directly put inside, and a cloth covering the keyboard that is perfectly cut to the size of the screen that (I think) it&#39;s meant to protect.</p>
<p><em>Update (May/13)</em>: Yesterday I ordered a maxed out specs 13&#39;&#39; MBPr. After dedicating almost an entire week to look for the &quot;right&quot; laptop, I finally settled on that one.</p>
<p>In the past 1.5 weeks I&#39;ve been using just my 2009 Intel Core 2 Solo (single core) Acer Aspire AS 1410, which, after adding 2 additional GB of RAM ($32 at Fry&#39;s, my favorite store) for a total of 4 GB, <em>and</em> upgrading to the latest Ubuntu 64 bit release, (14.04 LTS ftw!!... it had a 1 year old 32 bit release installed which was dead slow) has turned out to be amazingly fast and responsive considering the hardware limitations. I&#39;m just amazed at how much upping the RAM and using a 64 bit OS and applications helped unlock this baby&#39;s potential.</p>
<p>One of the reasons why I didn&#39;t want a MBP is that I felt that I was likely to be using Ubuntu as my primary OS anyway, given how good it has worked for me on this outdated hardware: I wasn&#39;t willing to pay the Apple premiun since I wasn&#39;t planning on using their OSX as my primary OS anyway. Long story short, I found the MBPr 13&#39;&#39; to be:</p>
<ol>
<li>Just <em>slightly</em> above my ~3 lb of weight limit.</li>
<li>Twice as much RAM as competing PC Ultrabooks that I could get fast (some custom configurations have it but would take an additional 2 to 3 weeks to ship). Considering that neither MBPr nor PC Ultrabooks are user upgradeable.</li>
<li>Twice as much SSD HDD storage capacity compared to PC Ultrabooks that I could get fast (same as the RAM).</li>
<li>1 GHZ faster clock speed on average compared to the Ultrabooks that I was considering (2.8 GHz vs ~1.8 GHz on the Ultrabooks).</li>
<li>Better Intel integrated video (Intel Iris 5000 series video vs Intel 4400 series video on most Ultrabooks, except for the Asus Zenbook which is only $300 chea... less expensive, but comes with half the SSD capacity, half the RAM).</li>
<li>Better by almost 4 hours extra battery life on light usage (with not much video streaming): ~9 hrs vs ~5 hrs for most Ultrabooks.</li>
<li>Easily run OSX, which I was planning to do either using VMs or remote desktop connecting to the Hackintosh desktop that I&#39;m planning to host at home.</li>
<li>Around $800 more expensive than Ultrabooks I was considering.</li>
</ol>
<p>One way I think of it is this: I&#39;m really getting 2 fully featured machines. I&#39;ll likely use OSX with an Ubuntu guest with 8GB of RAM during my day to day, switching back to the OSX host (with a full 8GB available to itself) whenever I would need to use commercial applications like Photoshop, some games, Apple&#39;s Video and Audio editing apps, etc.</p>
]]></description><link>http://www.gerardomoad.com/posts/on-macbook-pros</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/on-macbook-pros</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Tue, 13 May 2014 00:00:00 GMT</pubDate></item><item><title><![CDATA[PHP Composer]]></title><description><![CDATA[<p>PHP Composer dependency manager. I just found out about it. All I have to say is: &quot;Wow!&quot;.</p>
<p>I regularly go back to the <a href="http://www.doctrine-project.org/">Doctrine Project&#39;s website</a> to review their fabulous ORM&#39;s documentation and to find out if there are any new versions.</p>
<p>Around a week or two ago I went back to Doctrine&#39;s website and something caught my eye: they were no longer recommending downloading the tarballs/zip files, installing with PEAR, or checking out code directly from their source control repo. They were recommending people use what seemed to me like a new, non-standard way to install modules on PHP. I didn&#39;t see the need to upgrade then, so I didn&#39;t bother downloading the new versions either via the traditional methods that I was more familiar with or via this crazy new way.</p>
<p>It was only 2 days ago, while I was doing the usual Friday afternoon web surfing (and after I had just realized the existence of something called PHAR archive packages for PHP, kind of like Java JARs but for PHP) that I went back to Doctrine&#39;s site and I decided to follow the link to the <a href="https://packagist.org/packages/doctrine/orm">Composer distribution of the Doctrine ORM</a>. I was welcomed by a neat little page that looks everything like what <a href="http://pear.php.net">PHP.net&#39;s PEAR site</a> should look like.</p>
<p>Upon some browsing around the pages for the package that I was interested in, as well as others, I fell in love with <a href="http://www.packagist.org">Packagist</a>, the packaged modules repository that <a href="http://getcomposer.org/">Composer</a> uses by default.</p>
<p>So, basically, what these guys have done is to bring Web 2.0 (shouldn&#39;t that term be way past its expiration date now that we&#39;ve been using it for over 6 years?) toolset cleanness and openness to the PHP world, something that will hopefully help relive a lot of abandoned projects (license permitting, Composer&#39;s compatibility with Git, Mercurial and other DVCMS makes it easy for developers to continue working where somebody else left off, maybe a while ago!).</p>
<p>I expect to make a few more posts on this topic in the near future as I experiment with these new tools.</p>
]]></description><link>http://www.gerardomoad.com/posts/php-composer</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/php-composer</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Sun, 03 Feb 2013 00:00:00 GMT</pubDate></item><item><title><![CDATA[Redirect to file as sudo]]></title><description><![CDATA[<p>When you want to run a command as you or as some other regular user (i.e., not a superuser), but you need to redirect the output to a file that is/should be owned by root.</p>
<p>Say I have a script called <code>generate_apache_config</code>, which, given a domain name and a path (representing a site&#39;s webroot), generates an apache <code>.conf</code> file.</p>
<p>Furthermore, say we got this script from someone else, and we&#39;re too lazy to code-review it. In this case, the only sensible way to make use of the script is <strong>not to run it with superuser privileges</strong>.</p>
<p>The following doesn&#39;t work, because our user can&#39;t write a new file to the <code>/etc/apache2/available-sites/</code> directory:</p>
<pre><code class="hljs bash">generate_apache_config DOMAIN_NAME WEBROOT &gt; /etc/apache2/available-sites/DOMAIN_NAME.conf
    <span class="hljs-comment">#doesn't work because our regular user can't create a file under that directory</span></code></pre>

<p>One may think that executing the whole thing as sudo may make this work, but it won&#39;t, because only command <code>generate_apache_config</code> is invoked as sudo. <code>tee</code> is still invoked as our user:</p>
<pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> generate_apache_config DOMAIN_NAME WEBROOT &gt; /etc/apache2/available-sites/DOMAIN_NAME.conf
    <span class="hljs-comment">#doesn't work because redirecting to output file is done as regular user</span></code></pre>

<p>Use the <code>tee</code> command instead. Only the <code>tee</code> command is invoked with sudo:</p>
<pre><code class="hljs bash">generate_apache_config DOMAIN_NAME WEBROOT | <span class="hljs-built_in">sudo</span> tee /etc/apache2/available-sites/DOMAIN_NAME.conf
    <span class="hljs-comment">#works! we invoke our script with regular user privileges, and then redirect to tee, which we invoke as a super user with sudo</span></code></pre>]]></description><link>http://www.gerardomoad.com/posts/redirect-to-file-as-sudo</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/redirect-to-file-as-sudo</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Tue, 20 Nov 2012 00:00:00 GMT</pubDate></item><item><title><![CDATA[Doing HTTP PUT requests in PHP without using files]]></title><description><![CDATA[<p>I think it may be that because back when the <code>curl</code> PHP extension was first created, the <code>HTTP PUT</code> method still wasn&#39;t being used by almost anyone, the <code>API</code> for doing <code>PUTs</code> using PHP curl was implemented in a very strange way.</p>
<p>If you want to <code>PUT</code> some data generated programmatically in a PHP script, you&#39;re supposed to do:</p>
<pre><code class="hljs php"><span class="hljs-preprocessor">&lt;?php</span>
    <span class="hljs-variable">$data</span> = <span class="hljs-string">'dynamically generated data that we want to PUT to a webservice endpoint...'</span>;
    <span class="hljs-comment">// NOTE: you wouldn't hardcode the filename where you temporarily store your payload, you would</span>
    <span class="hljs-comment">//       use tmpfile() or tempname() to come up with a unique temporary file...</span>
    file_put_contents(<span class="hljs-string">'/tmp/file.txt'</span>, <span class="hljs-variable">$data</span>);

    <span class="hljs-variable">$ch</span> = curl_init();
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_URL, <span class="hljs-string">"http://example.com/endpoint.php"</span>);
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_PUT, <span class="hljs-keyword">true</span>);
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_INFILE, fopen(<span class="hljs-string">'/tmp/file.txt'</span>, <span class="hljs-string">'r'</span>));
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_INFILESIZE, filesize(<span class="hljs-string">'/tmp/file.txt'</span>));
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_RETURNTRANSFER, <span class="hljs-keyword">true</span>);
    <span class="hljs-variable">$curl_ret</span> = curl_exec(<span class="hljs-variable">$ch</span>);
    <span class="hljs-preprocessor">?&gt;</span></code></pre>

<p>That&#39;s so wrong on many levels. Here I enumerate a few:</p>
<ol>
<li>data is written to a file by a script only to be read back in by the same script!</li>
<li>an extra OS file <code>stat()</code> call will be made in order to get the size in bytes of the payload file.</li>
<li>you can mess things up badly if you don&#39;t use PHP&#39;s built-in <code>tmpfile()</code> or <code>tempname()</code> functions carefully to generate temporary files with unique random names.</li>
</ol>
<p>Of course, nowadays people very much like the <code>HTTP PUT</code> method again, and there has been a resurgence in its use ever since <code>REST</code> became a trend.</p>
<p>Thankfully, we can make more efficient <code>HTTP POSTs</code> of data generated by our scripts with PHP curl as follows:</p>
<pre><code class="hljs php"><span class="hljs-preprocessor">&lt;?php</span>
    <span class="hljs-variable">$data</span> = <span class="hljs-string">'dynamically generated data that we want to PUT to a webservice endpoint...'</span>;

    <span class="hljs-variable">$ch</span> = curl_init();
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_URL, <span class="hljs-string">'http://example.com/endpoint.php'</span>);
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_HTTPHEADER, <span class="hljs-keyword">array</span>(<span class="hljs-string">'content-type: application/json'</span>)); <span class="hljs-comment">// you may or may not need to do this, depending on what</span>
                                                                                         <span class="hljs-comment">// your endpoint expects the posted content-type to be</span>
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_CUSTOMREQUEST, <span class="hljs-string">'PUT'</span>);
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_POSTFIELDS, <span class="hljs-variable">$data</span>); <span class="hljs-comment">//you may have to urlencode() your data, but most likely you won't</span>
    <span class="hljs-variable">$ch</span> = curl_setopt(<span class="hljs-variable">$ch</span>, CURLOPT_RETURNTRANSFER, <span class="hljs-keyword">true</span>);
    <span class="hljs-variable">$curl_ret</span> = curl_exec(<span class="hljs-variable">$ch</span>);
    <span class="hljs-preprocessor">?&gt;</span></code></pre>

<p>With this approach, we are able to do the same <code>HTTP PUT</code> without incurring in performance penalties associated with excessive use of file <code>stat()</code> operations.</p>
]]></description><link>http://www.gerardomoad.com/posts/doing-http-put-requests-in-php-without-using-files</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/doing-http-put-requests-in-php-without-using-files</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Sat, 15 Sep 2012 00:00:00 GMT</pubDate></item><item><title><![CDATA[Backup only what's necessary]]></title><description><![CDATA[<p>When backing up a home directory, you don&#39;t want to backup subversion checkout subdirectories that have no changes relative to their repositories (i.e. subversion directories with no local changes).</p>
<p>I made a script which will help you find directories corresponding to svn checkouts that are up-to date relative to their repositories. I saved it as <code>~/bin/get_up_to_date_svn_dirs.pl</code>:</p>
<pre><code class="hljs perl"><span class="hljs-comment">#!/usr/bin/perl -w</span>

    <span class="hljs-keyword">my</span> <span class="hljs-variable">@input_list</span> = ();
    <span class="hljs-keyword">my</span> <span class="hljs-variable">@clean_paths</span> = ();
    <span class="hljs-keyword">my</span> <span class="hljs-variable">@clean_parents</span> = ();
    <span class="hljs-keyword">while</span>(&lt;&gt;) { 
        <span class="hljs-keyword">push</span> <span class="hljs-variable">@input_list</span>, <span class="hljs-variable">$_</span>;
    }

    <span class="hljs-keyword">my</span> <span class="hljs-variable">$cmp_path</span> = <span class="hljs-string">"cmp_path"</span>;

    <span class="hljs-comment"># sorts alphabetically/lexicographically based on the "path" component of the input line.</span>
    <span class="hljs-variable">@input_list</span> = <span class="hljs-variable">@clean_parents</span> = <span class="hljs-keyword">sort</span> cmp_path <span class="hljs-variable">@input_list</span>;

    <span class="hljs-keyword">for</span> <span class="hljs-keyword">my</span> <span class="hljs-variable">$line</span>(<span class="hljs-variable">@input_list</span>) {
        <span class="hljs-variable">$line</span> =~ <span class="hljs-regexp">m/^\s*(\d+)\s*(.*)\s*$/</span>;
        <span class="hljs-keyword">my</span> <span class="hljs-variable">$path</span> = <span class="hljs-variable">$2</span>;
        <span class="hljs-keyword">my</span> <span class="hljs-variable">$svn_info_out</span> = <span class="hljs-string">`cd $path; svn info &gt; /dev/null 2&gt;/dev/null; echo \$?;`</span>;
        <span class="hljs-keyword">chomp</span> <span class="hljs-variable">$svn_info_out</span>;
        <span class="hljs-keyword">if</span>(<span class="hljs-variable">$svn_info_out</span> eq <span class="hljs-string">"1"</span>) {
            <span class="hljs-keyword">next</span>;
        }
        <span class="hljs-keyword">my</span> <span class="hljs-variable">$svn_status_out</span> = <span class="hljs-string">`cd $path; svn status`</span>;
        <span class="hljs-keyword">chomp</span> <span class="hljs-variable">$svn_status_out</span>;
        <span class="hljs-keyword">if</span>(<span class="hljs-variable">$svn_status_out</span> eq <span class="hljs-string">""</span>) {
            <span class="hljs-keyword">if</span>(<span class="hljs-keyword">scalar</span>(<span class="hljs-variable">@clean_paths</span>) != <span class="hljs-number">0</span>) {
                <span class="hljs-keyword">my</span> <span class="hljs-variable">$prev_clean_path</span> = <span class="hljs-variable">$clean_paths</span>[<span class="hljs-keyword">scalar</span>(<span class="hljs-variable">@clean_paths</span>) - <span class="hljs-number">1</span>];
                <span class="hljs-keyword">if</span>(<span class="hljs-variable">$path</span> =~ <span class="hljs-regexp">m/^.*$prev_clean_path.*$/</span>) {
                    <span class="hljs-keyword">next</span>;
                }
            }
            <span class="hljs-keyword">push</span> <span class="hljs-variable">@clean_paths</span>, <span class="hljs-variable">$path</span>;
            <span class="hljs-keyword">print</span> <span class="hljs-string">"<span class="hljs-variable">$path</span>\n"</span>;
        }
    }

    <span class="hljs-comment"># comparison function takes 2 lines having the following format:</span>
    <span class="hljs-comment"># 1024    /tmp/a/file.txt</span>
    <span class="hljs-comment"># and returns the result of alphabetically comparing the second part of the line</span>
    <span class="hljs-comment"># (e.g. /tmp/a/file.txt)</span>
    <span class="hljs-sub"><span class="hljs-keyword">sub</span> cmp_path {</span>
        <span class="hljs-variable">$a</span> =~ <span class="hljs-regexp">m/^\s*(\d+)\s*(.*)\s*$/</span>;
        <span class="hljs-keyword">my</span> <span class="hljs-variable">$path_a</span> = <span class="hljs-variable">$2</span>;
        <span class="hljs-variable">$b</span> =~ <span class="hljs-regexp">m/^\s*(\d+)\s*(.*)\s*$/</span>;
        <span class="hljs-keyword">my</span> <span class="hljs-variable">$path_b</span> = <span class="hljs-variable">$2</span>;
        <span class="hljs-keyword">return</span> <span class="hljs-variable">$path_a</span> cmp <span class="hljs-variable">$path_b</span>;
    }</code></pre>


<p>First, I run <code>du</code>, and <code>sort</code> it numerically to get an idea of how big the home directory that I want to back up is, and save the output to file <code>user-du.txt</code> under my own home directory:</p>
<pre><code class="hljs perl">du ~user/ | <span class="hljs-keyword">sort</span> -n | tee ~<span class="hljs-regexp">/user-du.txt</span></code></pre>

<p>Now, I&#39;ll feed this <code>du</code> output to my script, which will generate a list of paths in the input file that correspond to svn checkout directories that are up to date relative to their repositories. I save this list in file <code>~/tmp/user-all-clean.txt</code>:</p>
<pre><code class="hljs perl">cat ~<span class="hljs-regexp">/user-du.txt | ~/bin</span><span class="hljs-regexp">/get_up_to_date_svn_dirs.pl | tee ~/tmp</span><span class="hljs-regexp">/user-all-clean.txt</span></code></pre>

<p>At this point, I know:</p>
<ul>
<li>the base directory that I want to back up (e.g. the home directory of a user that I want to back up, like <code>~user/</code>).</li>
<li>a list of paths that it&#39;s not necessary to back up because they&#39;re subversion checkouts with no local changes.</li>
</ul>
<p>I can use <code>tar</code>, using the <code>--exclude-from</code> option to finally backup what I want in gzipped tarball file <code>~/user-home.tgz</code></p>
<pre><code class="hljs perl">tar --exclude-from ~<span class="hljs-regexp">/tmp/user</span>-all-clean.txt -zcvf ~<span class="hljs-regexp">/user-home.tgz ~user/</span></code></pre>

<p>@TODO (exercise for the reader): Have <code>~/bin/get_up_to_date_svn_dirs.pl</code> output the svn <em>repository paths</em> and <em>revision numbers</em> that we&#39;re skipping to get the full information that would allow us to restore the backup precisely by restoring not only the tarball but also checking out the right <em>paths</em> and <em>revisions</em> from subversion.</p>
]]></description><link>http://www.gerardomoad.com/posts/backup-only-what-s-necessary</link><guid isPermaLink="true">http://www.gerardomoad.com/posts/backup-only-what-s-necessary</guid><dc:creator><![CDATA[Gerardo Moad]]></dc:creator><pubDate>Thu, 12 Apr 2012 00:00:00 GMT</pubDate></item></channel></rss>